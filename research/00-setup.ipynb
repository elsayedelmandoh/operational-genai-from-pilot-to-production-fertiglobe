{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4197a1b",
   "metadata": {},
   "source": [
    "# RAG in Production: From Prototype to Deployment\n",
    "\n",
    "[Fertiglobe](https://www.linkedin.com/company/fertiglobe/)\n",
    "\n",
    "[medium: Building and Evaluating your First RAG](https://medium.com/henkel-data-and-analytics/building-and-evaluating-your-first-rag-90da2c55e6ff)\n",
    "\n",
    "[repo: prod-rag](https://github.com/El-Moghazy2/prod-rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8d981",
   "metadata": {},
   "source": [
    "\n",
    "> **Based on:** [\"Building and Evaluating your First RAG\"](https://medium.com/henkel-data-and-analytics/building-and-evaluating-your-first-rag) by Abdelrhman ElMoghazy, Henkel Data & Analytics\n",
    "\n",
    "This 3-hour hands-on workshop expands the original article into a comprehensive, production-ready RAG system. We replace Azure OpenAI with **fully local Ollama** models for zero-cost execution.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. **Build a RAG pipeline** from scratch using LangGraph, FAISS, and Ollama\n",
    "2. **Add guardrails** for input validation, prompt injection detection, and output grounding\n",
    "3. **Engineer prompts** using 4 different strategies and compare them quantitatively\n",
    "4. **Optimize context** through chunk size tuning, k-value analysis, and re-ranking\n",
    "5. **Evaluate automatically** using RAGAS with automated test set generation\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- [Ollama](https://ollama.com) installed and running\n",
    "- Models pulled: `ollama pull llama3.2` and `ollama pull nomic-embed-text`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6aa2e",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0: Workshop Setup (~5 min)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c58a5",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Create and activate a conda environment before installing dependencies:\n",
    "\n",
    "```bash\n",
    "conda create -n rag-workshop python=3.10 -y\n",
    "conda activate rag-workshop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803528e",
   "metadata": {},
   "source": [
    "**Or** using pip with a virtual environment:\n",
    "\n",
    "```bash\n",
    "python -m venv rag-workshop\n",
    "# Windows (PowerShell)\n",
    "Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy RemoteSigned\n",
    "rag-workshop\\Scripts\\activate\n",
    "# macOS / Linux\n",
    "source rag-workshop/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c9384",
   "metadata": {},
   "source": [
    "**Or** using [Hatch](https://hatch.pypa.io):\n",
    "\n",
    "```bash\n",
    "pip install hatch\n",
    "hatch env create\n",
    "hatch shell\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb926c36",
   "metadata": {},
   "source": [
    "### Install Ollama\n",
    "\n",
    "Download and install Ollama from [ollama.com/download](https://ollama.com/download), then pull the required models:\n",
    "\n",
    "```bash\n",
    "ollama serve        # start the Ollama server (keep running in a separate terminal)\n",
    "ollama pull llama3.2           # LLM for generation\n",
    "ollama pull nomic-embed-text   # embedding model for retrieval\n",
    "ollama pull llama-guard3       # content safety classifier (Section 2 guardrails)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0262be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install all dependencies\n",
    "%pip install langchain langchain-ollama langchain-community langgraph faiss-cpu \\\n",
    "    PyMuPDF requests ragas python-dotenv openpyxl pandas matplotlib numpy pydantic rank-bm25 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a45fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running! Available models: ['llama-guard3:latest', 'nomic-embed-text:latest', 'llama3.2:latest']\n",
      "  llama3.2: FOUND\n",
      "  nomic-embed-text: FOUND\n"
     ]
    }
   ],
   "source": [
    "# Verify Ollama is running and models are available\n",
    "import requests\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=5)\n",
    "    response.raise_for_status()\n",
    "    models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "    print(f\"Ollama is running! Available models: {models}\")\n",
    "    \n",
    "    required = [\"llama3.2\", \"nomic-embed-text\"]\n",
    "    for model in required:\n",
    "        found = any(model in m for m in models)\n",
    "        status = \"FOUND\" if found else \"MISSING - run: ollama pull \" + model\n",
    "        print(f\"  {model}: {status}\")\n",
    "except requests.ConnectionError:\n",
    "    print(\"ERROR: Ollama is not running! Start it with: ollama serve\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
