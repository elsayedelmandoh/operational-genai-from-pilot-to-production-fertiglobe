{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a589ad58",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Context Engineering (~25 min)\n",
    "---\n",
    "\n",
    "### 4.1 Context Engineering Fundamentals\n",
    "\n",
    "**Context engineering** is about optimizing *what information reaches the LLM*. Even with a perfect prompt, garbage context = garbage answers.\n",
    "\n",
    "**Three levers to optimize:**\n",
    "\n",
    "1. **Chunk size**: How big each piece of context is\n",
    "2. **Retrieval count (k)**: How many chunks to retrieve\n",
    "3. **Re-ranking**: Reorder results by relevance after retrieval\n",
    "\n",
    "**The \"lost in the middle\" problem:**\n",
    "LLMs tend to focus on the beginning and end of their context, losing information in the middle. This means:\n",
    "- Fewer, more relevant chunks > many loosely relevant chunks\n",
    "- Re-ranking to put the best chunk first matters\n",
    "\n",
    "**Context window budget:**\n",
    "```\n",
    "Total context window (e.g., 8K tokens)\n",
    "  - System prompt:    ~200 tokens\n",
    "  - Retrieved context: ~2000-4000 tokens (our budget)\n",
    "  - Generation space:  ~2000 tokens\n",
    "  - Safety margin:     ~1000 tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c9735",
   "metadata": {},
   "source": [
    "### 4.2 Chunk Size Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fad04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   small:  1866 chunks | avg    146 chars | min    1 | max  200\n",
      "  medium:   702 chunks | avg    404 chars | min   40 | max  500\n",
      "   large:   352 chunks | avg    815 chars | min   58 | max  999\n",
      "  xlarge:   180 chunks | avg   1594 chars | min   62 | max 1999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "chunk_configs = {\n",
    "    \"small\": {\"chunk_size\": 200, \"chunk_overlap\": 40},\n",
    "    \"medium\": {\"chunk_size\": 500, \"chunk_overlap\": 100},\n",
    "    \"large\": {\"chunk_size\": 1000, \"chunk_overlap\": 200},\n",
    "    \"xlarge\": {\"chunk_size\": 2000, \"chunk_overlap\": 400},\n",
    "}\n",
    "\n",
    "chunk_stores = {}\n",
    "for name, config in chunk_configs.items():\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config[\"chunk_size\"],\n",
    "        chunk_overlap=config[\"chunk_overlap\"],\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    config_chunks = splitter.split_documents(documents)\n",
    "    store = FAISS.from_documents(config_chunks, embeddings)\n",
    "    chunk_stores[name] = {\"store\": store, \"chunks\": config_chunks}\n",
    "    \n",
    "    sizes = [len(c.page_content) for c in config_chunks]\n",
    "    print(f\"{name:>8}: {len(config_chunks):>5} chunks | avg {np.mean(sizes):>6.0f} chars | \"\n",
    "          f\"min {min(sizes):>4} | max {max(sizes):>4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce3d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"What PPE is required for handling DESMOPHEN XP 2680?\"\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "\n",
    "for name, data in chunk_stores.items():\n",
    "    results = data[\"store\"].similarity_search_with_score(test_query, k=3)\n",
    "    avg_score = np.mean([score for _, score in results])\n",
    "    total_chars = sum(len(doc.page_content) for doc, _ in results)\n",
    "    \n",
    "    print(f\"{name:>8}: avg_distance={avg_score:.4f} | total_chars={total_chars:>5} | \"\n",
    "          f\"chunks_returned={len(results)}\")\n",
    "    for i, (doc, score) in enumerate(results):\n",
    "        print(f\"          [{i+1}] distance={score:.4f} | {len(doc.page_content)} chars | \"\n",
    "              f\"{doc.metadata.get('product_name', '?')}/S{doc.metadata.get('section_number', '?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88fdd2",
   "metadata": {},
   "source": [
    "### 4.3 Retrieval Parameter Tuning (k-value analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the impact of k on retrieval\n",
    "\n",
    "def analyze_k_impact(store, query: str, k_values: list[int]):\n",
    "    \"\"\"Test different k values and compare results.\"\"\"\n",
    "    print(f\"Query: '{query}'\\n\")\n",
    "    print(f\"{'k':>3} | {'Docs':>4} | {'~Tokens':>7} | {'Unique Sources':>14} | {'Avg Distance':>12}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for k in k_values:\n",
    "        results = store.similarity_search_with_score(query, k=k)\n",
    "        total_chars = sum(len(doc.page_content) for doc, _ in results)\n",
    "        approx_tokens = total_chars // 4  # rough estimate\n",
    "        unique_sources = len(set(\n",
    "            f\"{doc.metadata.get('product_name', '?')}/S{doc.metadata.get('section_number', '?')}\"\n",
    "            for doc, _ in results\n",
    "        ))\n",
    "        avg_dist = np.mean([score for _, score in results])\n",
    "        \n",
    "        print(f\"{k:>3} | {len(results):>4} | {approx_tokens:>7} | {unique_sources:>14} | {avg_dist:>12.4f}\")\n",
    "\n",
    "# Use the large chunk store (our default)\n",
    "analyze_k_impact(\n",
    "    chunk_stores[\"large\"][\"store\"],\n",
    "    \"What PPE is required for handling DESMOPHEN XP 2680?\",\n",
    "    k_values=[1, 3, 5, 10]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d6b4e",
   "metadata": {},
   "source": [
    "**Tradeoff analysis:**\n",
    "- **k=1**: Minimal context, highest precision, might miss information\n",
    "- **k=3**: Good balance (our default)\n",
    "- **k=5**: More context, higher recall, more noise\n",
    "- **k=10**: Maximum recall, but risk of \"lost in the middle\" and high cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d103f33a",
   "metadata": {},
   "source": [
    "### 4.4 Metadata Filtering & Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metadata Filtering ---\n",
    "# Filter retrieval to a specific product\n",
    "\n",
    "def retrieve_with_product_filter(store, query: str, product_name: str, k: int = 3):\n",
    "    \"\"\"Retrieve docs filtered by product name metadata.\"\"\"\n",
    "    # FAISS doesn't support native filtering, so we over-retrieve and filter\n",
    "    all_results = store.similarity_search_with_score(query, k=k * 5)\n",
    "    filtered = [(doc, score) for doc, score in all_results if product_name.lower() in doc.metadata.get(\"product_name\", \"\").lower()]\n",
    "    return filtered[:k]\n",
    "\n",
    "query = \"What are the hazardous decomposition products?\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"--- Unfiltered (all products) ---\")\n",
    "for doc, score in vector_store.similarity_search_with_score(query, k=3):\n",
    "    print(f\"  {doc.metadata.get('product_name', '?')}/S{doc.metadata.get('section_number', '?')} | distance={score:.4f}\")\n",
    "\n",
    "print(\"\\n--- Filtered (BAYBLEND M750 only) ---\")\n",
    "for doc, score in retrieve_with_product_filter(vector_store, query, product_name=\"BAYBLEND M750\", k=3):\n",
    "    print(f\"  {doc.metadata.get('product_name', '?')}/S{doc.metadata.get('section_number', '?')} | distance={score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c70c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based Re-ranking\n",
    "# Use the LLM to score and re-order retrieved results\n",
    "\n",
    "def rerank_results(query: str, docs: list, llm, top_n: int = 3) -> list:\n",
    "    \"\"\"Re-rank documents by LLM-judged relevance (1-10).\"\"\"\n",
    "    scored = []\n",
    "    for doc in docs:\n",
    "        prompt = (\n",
    "            f\"Rate the relevance of this document to the query on a scale of 1-10.\\n\\n\"\n",
    "            f\"Query: {query}\\n\\n\"\n",
    "            f\"Document: {doc.page_content[:500]}\\n\\n\"\n",
    "            f\"Respond with ONLY a single number from 1 to 10.\"\n",
    "        )\n",
    "        response = llm.invoke(prompt)\n",
    "        try:\n",
    "            score = int(response.content.strip())\n",
    "        except ValueError:\n",
    "            score = 5  # default if parsing fails\n",
    "        scored.append((doc, score))\n",
    "    \n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_n]\n",
    "\n",
    "# Compare basic vs re-ranked retrieval\n",
    "query = \"What are the storage requirements for DESMOPHEN XP 2680?\"\n",
    "basic_results = vector_store.similarity_search(query, k=5)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"--- Basic retrieval (top 5 by vector distance) ---\")\n",
    "for i, doc in enumerate(basic_results, 1):\n",
    "    print(f\"  [{i}] {doc.metadata.get('product_name', '?')}/S{doc.metadata.get('section_number', '?')} | {doc.page_content[:80]}...\")\n",
    "\n",
    "reranked = rerank_results(query, basic_results, llm, top_n=3)\n",
    "print(\"\\n--- Re-ranked (top 3 by LLM relevance) ---\")\n",
    "for i, (doc, score) in enumerate(reranked, 1):\n",
    "    print(f\"  [{i}] score={score}/10 | {doc.metadata.get('product_name', '?')}/S{doc.metadata.get('section_number', '?')} | {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d835b3b1",
   "metadata": {},
   "source": [
    "### 4.5 Hybrid Search (BM25 + Vector)\n",
    "\n",
    "Vector search excels at **semantic similarity** â€” finding chunks that *mean* the same thing even with different wording. But it can miss exact keyword matches that a user expects.\n",
    "\n",
    "**BM25** is a classic keyword-based ranking algorithm (used by Elasticsearch, Lucene, etc.). It excels at:\n",
    "- Exact term matching (\"BERT\", \"BPE\", \"tokenizer\")\n",
    "- Queries with rare or specific technical terms\n",
    "- Cases where the user's wording closely matches the document\n",
    "\n",
    "**Hybrid search** combines both:\n",
    "\n",
    "| Strategy | Strengths | Weaknesses |\n",
    "|----------|-----------|------------|\n",
    "| **Vector** | Semantic understanding, paraphrases | Misses exact keywords |\n",
    "| **BM25** | Exact term matching, rare terms | No semantic understanding |\n",
    "| **Hybrid** | Best of both worlds | Slightly more compute |\n",
    "\n",
    "We use LangChain's `EnsembleRetriever` which merges results using **Reciprocal Rank Fusion (RRF)** with configurable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.vectorstore import create_retriever\n",
    "\n",
    "# Create all 3 retriever strategies\n",
    "vector_ret = create_retriever(\"vector\", chunks, vector_store, k=3)\n",
    "bm25_ret = create_retriever(\"bm25\", chunks, vector_store, k=3)\n",
    "hybrid_ret = create_retriever(\"hybrid\", chunks, vector_store, k=3)\n",
    "\n",
    "# Compare results on a query with specific technical terms\n",
    "query = \"What is the GHS classification and CAS numbers for DESMOPHEN XP 2680?\"\n",
    "print(f\"Query: '{query}'\")\n",
    "\n",
    "for name, ret in [(\"Vector\", vector_ret), (\"BM25\", bm25_ret), (\"Hybrid\", hybrid_ret)]:\n",
    "    docs = ret.invoke(query)\n",
    "    print(f\"--- {name} ({len(docs)} docs) ---\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        preview = doc.page_content[:120].replace(chr(10), ' ')\n",
    "        print(f\"  [{i}] {doc.metadata.get('product_name', '?')}/S{doc.metadata.get('section_number', '?')} | {preview}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d97fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What is BPE tokenization and how does BERT use it?'\n",
      "                             Pure Vector: ['ch2/p4', 'ch2/p7', 'ch3/p2', 'ch3/p3', 'ch3/p2', 'ch1/p5']\n",
      "         30% BM25 / 70% Vector (default): ['ch2/p4', 'ch2/p7', 'ch3/p2', 'ch3/p3', 'ch3/p2', 'ch1/p5']\n",
      "                   50% BM25 / 50% Vector: ['ch3/p3', 'ch2/p4', 'ch3/p2', 'ch2/p7', 'ch1/p5', 'ch3/p2']\n",
      "                   70% BM25 / 30% Vector: ['ch3/p3', 'ch3/p2', 'ch1/p5', 'ch2/p4', 'ch2/p7', 'ch3/p2']\n",
      "                               Pure BM25: ['ch3/p3', 'ch3/p2', 'ch1/p5', 'ch2/p4', 'ch2/p7', 'ch3/p2']\n"
     ]
    }
   ],
   "source": [
    "# Show how adjusting weights changes hybrid results\n",
    "\n",
    "weight_configs = [\n",
    "    (0.0, 1.0, \"Pure Vector\"),\n",
    "    (0.3, 0.7, \"30% BM25 / 70% Vector (default)\"),\n",
    "    (0.5, 0.5, \"50% BM25 / 50% Vector\"),\n",
    "    (0.7, 0.3, \"70% BM25 / 30% Vector\"),\n",
    "    (1.0, 0.0, \"Pure BM25\"),\n",
    "]\n",
    "\n",
    "query = \"What is BPE tokenization and how does BERT use it?\"\n",
    "print(f\"Query: '{query}'\")\n",
    "\n",
    "for bm25_w, vec_w, label in weight_configs:\n",
    "    ret = create_retriever(\"hybrid\", chunks, vector_store, k=3,\n",
    "                           bm25_weight=bm25_w, vector_weight=vec_w)\n",
    "    docs = ret.invoke(query)\n",
    "    sources = [f\"ch{d.metadata.get('chapter')}/p{d.metadata.get('page')}\" for d in docs]\n",
    "    print(f\"{label:>40}: {sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07630cb",
   "metadata": {},
   "source": [
    "### 4.6 Building the Optimized Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4b50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RAG graph compiled (large chunks + k=5 + re-ranking)!\n"
     ]
    }
   ],
   "source": [
    "# Combine: large chunks + k=5 + re-ranking into an optimized graph\n",
    "\n",
    "optimized_store = chunk_stores[\"large\"][\"store\"]\n",
    "\n",
    "def optimized_retrieve(state: State) -> dict:\n",
    "    \"\"\"Retrieve with re-ranking.\"\"\"\n",
    "\n",
    "    docs = optimized_store.similarity_search(state[\"question\"], k=5)\n",
    "\n",
    "    reranked = rerank_results(state[\"question\"], docs, llm, top_n=3)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc, _ in reranked)\n",
    "    return {\"context\": context}\n",
    "\n",
    "def optimized_generate(state: State) -> dict:\n",
    "    messages = selected_prompt.format_messages(\n",
    "        context=state[\"context\"],\n",
    "        question=state[\"question\"],\n",
    "    )\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "opt_builder = StateGraph(State)\n",
    "opt_builder.add_node(\"retrieve\", optimized_retrieve)\n",
    "opt_builder.add_node(\"generate\", optimized_generate)\n",
    "opt_builder.add_edge(START, \"retrieve\")\n",
    "opt_builder.add_edge(\"retrieve\", \"generate\")\n",
    "opt_builder.add_edge(\"generate\", END)\n",
    "\n",
    "optimized_graph = opt_builder.compile()\n",
    "print(\"Optimized RAG graph compiled (large chunks + k=5 + re-ranking)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimized pipeline\n",
    "result = optimized_graph.invoke({\"question\": \"What PPE is required for DESMOPHEN XP 2680?\"})\n",
    "print(\"Question:\", result[\"question\"])\n",
    "print(\"\\nAnswer:\", result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
