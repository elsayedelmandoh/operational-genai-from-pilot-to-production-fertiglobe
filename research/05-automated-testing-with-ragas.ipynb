{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11459a6d",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Automated Testing with RAGAS (~25 min)\n",
    "---\n",
    "\n",
    "### 5.1 Why Automated Evaluation\n",
    "\n",
    "Manual testing doesn't scale. **RAGAS** (Retrieval-Augmented Generation Assessment) automates evaluation using **LLM-as-judge**.\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| Manual evaluation | High quality, nuanced | Slow, expensive, inconsistent |\n",
    "| Automated (RAGAS) | Fast, reproducible, scalable | Depends on judge LLM quality |\n",
    "\n",
    "**Three metrics we'll use:**\n",
    "\n",
    "| Metric | What it measures | Score meaning |\n",
    "|--------|-----------------|---------------|\n",
    "| **LLMContextRecall** | Does retrieved context contain needed info? | 1.0 = all needed info retrieved |\n",
    "| **Faithfulness** | Is answer faithful to context (no hallucination)? | 1.0 = fully grounded |\n",
    "| **FactualCorrectness** | Does answer match ground truth? | 1.0 = perfectly correct |\n",
    "\n",
    "### RAGAS Test Set Generation\n",
    "\n",
    "Instead of manually creating test questions, RAGAS can **automatically generate a test set** from your documents using a KnowledgeGraph. This ensures comprehensive coverage of your SDS content. We also include a hand-crafted set of 12 SDS Q&A pairs in `qa_dataset.xlsx` for targeted evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34816660",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.2 Building a Knowledge Graph & Generating the Test Set\n",
    "\n",
    "R#AGAS builds a **KnowledgeGraph** from your documents, applies transformations to understand relationships, then synthesizes diverse test questions automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deacd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAGAS evaluation with Ollama as judge\n",
    "\n",
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    FactualCorrectness(llm=evaluator_llm),\n",
    "]\n",
    "\n",
    "ragas_result = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings,\n",
    ")\n",
    "\n",
    "print(\"RAGAS Evaluation Results:\")\n",
    "print(f\"  LLM Context Recall: {ragas_result['context_recall']:.4f}\")\n",
    "print(f\"  Faithfulness:       {ragas_result['faithfulness']:.4f}\")\n",
    "print(f\"  Factual Correctness:{ragas_result['factual_correctness']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View per-question results\n",
    "results_df = ragas_result.to_pandas()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc7b86",
   "metadata": {},
   "source": [
    "**Interpreting the scores:**\n",
    "\n",
    "- **LLMContextRecall > 0.7**: Our retrieval is finding relevant information\n",
    "- **Faithfulness > 0.8**: The LLM is staying grounded (thanks to our restrictive prompt)\n",
    "- **FactualCorrectness**: Depends heavily on question difficulty and context coverage\n",
    "\n",
    "Low scores indicate areas for improvement - either in retrieval (chunk size, k) or generation (prompt engineering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e51997",
   "metadata": {},
   "source": [
    "### 5.4 Comparative Evaluation Pipeline\n",
    "\n",
    "Now let's compare multiple RAG configurations to find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graphs with different retriever strategies using the factory\n",
    "from rag.vectorstore import create_retriever\n",
    "from rag.pipeline import build_basic_graph\n",
    "\n",
    "# Create retrievers\n",
    "vector_retriever = create_retriever(\"vector\", chunks, vector_store, k=3)\n",
    "bm25_retriever = create_retriever(\"bm25\", chunks, vector_store, k=3)\n",
    "hybrid_retriever = create_retriever(\"hybrid\", chunks, vector_store, k=3)\n",
    "\n",
    "# Build a graph for each retriever strategy\n",
    "vector_graph = build_basic_graph(llm, vector_store, prompt_template=selected_prompt, k=3,\n",
    "                                  retriever=vector_retriever)\n",
    "bm25_graph = build_basic_graph(llm, vector_store, prompt_template=selected_prompt, k=3,\n",
    "                                retriever=bm25_retriever)\n",
    "hybrid_graph = build_basic_graph(llm, vector_store, prompt_template=selected_prompt, k=3,\n",
    "                                  retriever=hybrid_retriever)\n",
    "\n",
    "# Evaluate each retriever strategy\n",
    "print(\"Evaluating retriever strategies...\")\n",
    "all_results.append(evaluate_config(vector_graph, \"Vector Retriever\", testset_df))\n",
    "all_results.append(evaluate_config(bm25_graph, \"BM25 Retriever\", testset_df))\n",
    "all_results.append(evaluate_config(hybrid_graph, \"Hybrid Retriever\", testset_df))\n",
    "\n",
    "print(\"Done! All 6 configurations evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ac8cb1",
   "metadata": {},
   "source": [
    "### 5.5 Workshop Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "## What We Built\n",
    "\n",
    "In this workshop, we built a **production-ready RAG system** with:\n",
    "\n",
    "1. **Data Pipeline**: PDF extraction (PyMuPDF) → section splitting → embedding → FAISS indexing\n",
    "2. **Guardrails**: Input validation, prompt injection detection, topic relevance, output grounding\n",
    "3. **Prompt Engineering**: 4 strategies compared quantitatively (restrictive, permissive, few-shot, structured)\n",
    "4. **Context Engineering**: Chunk size optimization, k-value analysis, LLM-based re-ranking\n",
    "5. **Hybrid Search**: BM25 + vector retrieval with configurable weights via EnsembleRetriever\n",
    "6. **Evaluation**: Automated RAGAS pipeline with KnowledgeGraph-based test set generation, comparing prompt strategies and retriever strategies side by side\n",
    "\n",
    "## Next Steps for Production\n",
    "\n",
    "- **CI/CD with RAGAS**: Run evaluation on every code change to catch regressions\n",
    "- **Monitoring**: Track latency, error rates, and user satisfaction in production\n",
    "- **Conversation Memory**: Add multi-turn context for follow-up questions\n",
    "- **Advanced Re-ranking**: Use cross-encoder models for better re-ranking\n",
    "- **Semantic Chunking**: Split documents by meaning instead of fixed character count\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Original Article: Building and Evaluating your First RAG](https://medium.com/henkel-data-and-analytics/building-and-evaluating-your-first-rag) by Abdelrhman ElMoghazy\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [RAGAS Documentation](https://docs.ragas.io/)\n",
    "- [Ollama](https://ollama.com)\n",
    "- [Covestro Product Safety](https://www.productsafetyfirst.covestro.com)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
